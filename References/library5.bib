@book{Arora2009,
  title = {Computational {{Complexity}}: {{A Modern Approach}}},
  author = {Arora, Sanjeev and Barak, Boaz},
  date = {2009},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  url = {https://doi.org/10.1017/CBO9780511804090},
  abstract = {This beginning graduate textbook describes both recent achievements and classical results of computational complexity theory. Requiring essentially no background apart from mathematical maturity, the book can be used as a reference for self-study for anyone interested in complexity, including physicists, mathematicians, and other scientists, as well as a textbook for a variety of courses and seminars. More than 300 exercises are included with a selected hint set. The book starts with a broad introduction to the field and progresses to advanced results. Contents include: definition of Turing machines and basic time and space complexity classes, probabilistic algorithms, interactive proofs, cryptography, quantum computation, lower bounds for concrete computational models (decision trees, communication complexity, constant depth, algebraic and monotone circuits, proof complexity), average-case complexity and hardness amplification, derandomization and pseudorandom constructions, and the PCP theorem.},
  isbn = {978-0-521-42426-4}
}

@book{Mohri2018,
  title = {Foundations of Machine Learning},
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  date = {2018},
  edition = {2},
  publisher = {MIT Press},
  isbn = {978-0-262-03940-6}
}

@inproceedings{Vyas2023,
  title = {On Provable Copyright Protection for Generative Models},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  author = {Vyas, Nikhil and Kakade, Sham M. and Barak, Boaz},
  editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  date = {2023-07-23/2023-07-29},
  series = {Proceedings of Machine Learning Research},
  volume = {202},
  pages = {35277--35299},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v202/vyas23b.html},
  abstract = {There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data C that was in their training set. We give a formal definition of near access-freeness (NAF) and prove bounds on the probability that a model satisfying this definition outputs a sample similar to C, even if C is included in its training set. Roughly speaking, a generative model p is k-NAF if for every potentially copyrighted data C, the output of p diverges by at most k-bits from the output of a model q that did not access C at all. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.},
  keywords = {ObsCite}
}
