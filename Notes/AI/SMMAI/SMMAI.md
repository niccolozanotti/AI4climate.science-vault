---
title: Statistical and Mathematical Methods for AI
date: 2024-09-22
Year: 1
Semester: 1
Professor: 
  - Elena Loli Piccolomini
  - Davide Evangelista
Credits: 6
URL: >
  https://www.unibo.it/en/study/phd-professional-masters-specialisation-schools-and-other-programmes/course-unit-catalogue/course-unit/2024/446599
tags: 
 - course
aliases: 
description: >
  At the end of the course, the student masters the basic mathematical and statistical methods needed to acquire skills in artificial intelligence foundations, theory and applications.
---
## Course contents

>[!summary]- Theory module contents
> 1. Elements of **Linear Algebra**
> 	- [[SMMAI Linear Algebra#Matrices|Matrices]]
> 	- [[SMMAI Linear Algebra#Solving systems of linear equations|Solving systems of linear equations]]
> 	- [[SMMAI Linear Algebra#Vector Spaces|Vector Spaces]]
> 	- [[SMMAI Linear Algebra#Linear independence|Linear independence]]
> 	- [[SMMAI Linear Algebra#Basis and Rank|Basis and Rank]]
> 	- [[SMMAI Linear Algebra#Linear Mappings|Linear Mappings]]
> 	- [[SMMAI Linear Algebra#Affine Spaces|Affine Spaces]]
> 	
> 2. **Analytic Geometry**
> 3. **Matrix Decompositions**
> 	
> 4. Elements of **Multivariate Analysis** 
> 	- Gradient, Jacobian, Hessian. Taylor theorem.
> 	- Convex functions and sets.
> 	
> 5. **Multivariate Optimization**
> 	- Linear least squares.
> 	- Extrema of multivariate functions. Optimality conditions.
> 	- Descent methods. Gradient type methods and Newton type methods.
> 	- Regularization.
> 	- Basis concepts of stochastic optimization.
> 
> 6. Elements of probability and statistics.
> 	- Probability and Bayes theorem.
> 	- Random variables. Continuous and discrete distributions of random variables. Normal and Poisson distributions. 
> 	- Independent and dependent variables. Covariance and correlation.
> 	- Estimates: Maximum Likelihood and Maximum a Posteriori estimates.
> 	- Cross entropy and Kullback-Leibler divergence.
^syllabus-SMMAI-theory

>[!summary]- Lab module contents
> - [[SMMAI-labs Lecture1|Lecture 1]] : Introduction to `numpy`
> - [[SMMAI-labs Lecture2|Lecture 2]]: Plotting with `matplotlib`
> - [[SMMAI-labs Lecture3|Lecture 3]]: Introduction to the solution of linear systems using `numpy` 
> - [[SMMAI-labs Lecture4|Lecture 4]]: A (very short) introduction to Machine Learning
> - [[SMMAI-labs Lecture5|Lecture 5]]: Data compression with SVD
> - [[SMMAI-labs Lecture6|Lecture 6]]: Dimensionality Reduction with PCA
> - [[SMMAI-labs Lecture7|Lecture 7]]: Optimization with Gradient Descent
> - [[SMMAI-labs Lecture8|Lecture 8]]: Stochastic Gradient Descent
> - [[SMMAI-labs Lecture9|Lecture 9]]: Linear and Polynomial Regression
> - [[SMMAI-labs Lecture10|Lecture 10]]: MLE and MAP
> - [[SMMAI-labs Lecture11|Lecture 11]]: Logistic regression (for [[SMMAI Homework 3|Homework 3]])
^syllabus-SMMAI-lab
## Homework assignments

>[!summary]- Homework assignments
> - [[SMMAI Homework 1|Homework 1]]: Linear Algebra and Floating Point Arithmetic  
> - [[SMMAI Homework 2|Homework 2]]: SVD and PCA for Machine Learning
> - [[SMMAI Homework 3|Homework 3]]: Optimization
> - [[SMMAI Homework 4|Homework 4]]: MLE and MAP
^homework-list-SMMAI
## Exam

It is **mandatory** to complete the homework assigned in the Laboratory lessons to have the exam.  The exam consists in a written test and a brief oral discussion about the homework assignments.
The final score is the sum of:
- the score of the written test (maximum 22/30)
- the score of the oral exam on the assigned homework (maximum 10/30)
If the final score is greater than 30, the laude is assigned.

Oral questions [[SMMAI questions oral|here]].
## Reading material

>[!reading]
> - [[@Deisenroth2020|M.P. Deisenroth, A.A. Faisal, C.S. Ong (2020)]] (main textbook)
## Resources

- [Course web page](https://www.unibo.it/en/study/phd-professional-masters-specialisation-schools-and-other-programmes/course-unit-catalogue/course-unit/2024/446599)
- [Tutor teaching material](https://devangelista2.github.io/statistical-mathematical-methods/intro.html)

