---
title: >
  Lecture 9: Linear and Polynomial Regression
aliases: 
date: 2024-08-24
tags: 
description: Linear and Polynomial Regression
---
Additional links: 
 - Original [class material](https://www.evangelistadavide.com/teaching/)

<< [[SMMAI-labs Lecture8|Previous lecture]] | [[SMMAI-labs Lecture10|Next lecture]] >>

---
#todo clean
## Regression in Machine Learning

![[ML_diagram.png]]
Regression in a group of supervised learning techniques, alternative to classification. In classification, given a datapoint $x \in \mathbb{R}^d$, the task was to learn a model $y = f_\theta(x)$ such that $y$ represents one of the $K$ possible classes in which $x$ lies. On the other side, a Machine Learning problem is a Regression when the target variable $y \in \mathbb{R}$ is a **continuous** variable, and the task is to approximately *interpolate* between the $y$ values, with the intent of being able to predict new outcome of $y$ when a new $x$ is given as input.
![[regression_vs_classification.png]]
A part from that, the computation procedure to define a regression model is similar to the procedure required for a classification model. Indeed, let $f_\theta(x)$ be a parametric function of $x \in \mathbb{R}^d$, parameterized by $\theta \in \mathbb{R}^s$. Given a dataset

$$
    X = [x^1 x^2 \dots x^N] \in \mathbb{R}^{d \times N}
$$
$$
    Y = [y^1 y^2 \dots y^N] \in \mathbb{R}^N
$$
the task is to train the model such that $f_\theta(x^i) \approx y^i$ for any $i = 1, \dots, N$. Clearly, this is done by solving the optimization problem
$$
    \theta^* = \arg\min_{\theta \in \mathbb{R}^s} \ell(\theta; X, Y)
$$
where the specific form of the loss function $\ell(\theta; X, Y)$ will be given in the next post. When $d>1$, $f_\theta(x)$ is called **multivariate regression model**, while if $d=1$, $f_\theta(x)$ is an **univariate regression model**. For simplicity, from now on we will work with univariate regression models, where $x^i \in \mathbb{R}$ for any $i = 1, \dots, N$, and consequently $X = [x^1 x^2 \dots x^N] \in \mathbb{R}^N$.

## Linear Regression
The simplest regression model we can think of is the Linear Regression Model, defined as

$$
    f_\theta(x) = \theta_1 + \theta_2 x = \theta^T \hat{x}
$$

where $\theta = [\theta_1, \theta_2]^T$ and $\hat{x} = [1, x]^T$. When $\theta$ is optimized, it defines a straight line approximating the data. Clearly, the resulting model will be correct if and only if the real function $y = f(x)$ generating the data is linear. Unfortunately, even in situations where $f(x)$ can be considered linear, the collected data $y$ is always corrupted by noise, which consequently breaks the linearity.

Consider for example a situation where $y = f(x) = 2x$ is the model generating the data. The noiseless data $y$, together with the line generated by the prediction model $f_\theta(x)$, in that situation will look like that
![[regression_example0.png]]

Unfortunately, since the data is always corrupted by noise, the collected data will be more like
![[regression_example1.png]]

In this scenario, if $f_\theta(x) = \theta^T \hat{x}$ as in the assumption above, then we hope to find parameters $\theta$ such that the resulting line will be close to the original one (first figure), someway ignoring the noise present in the data. Luckily, at least when the noise is Gaussian distributed, it usually does the job. Indeed the resulting linear regression model from the image above is
![[regression_example2.png]]
which is almost equal to the real line from the first image.

## Polynomial Regression
In most of real life scenario, the linear regression model is too rigid to correctly approximate the data. Indeed, suppose to collect a dataset like
![[regression_example3.png]]
where it is clear that no straight line is able to correctly approximate it. Indeed, if we try to approximate it with a linear regression model, we obtain
![[regression_example5.png]]
When this is the case, it is required to define a more flexible model.

Given a number $K > 0$, define the regression model 
$$
    f_\theta(x) = \sum_{j=0}^K \phi_j(x) \theta_j
$$
where the functions $\phi_1, \phi_2, \dots, \phi_K$ are called **feature vectors**. Note that, for $K = 2$ and $\phi_0(x) = 1$, $\phi_1(x) = x$, we recover the linear regression model $f_\theta(x) = \theta_1 + \theta_2 x$. For different values of $K$ and different feature vectors $\phi_j(x)$, we get different regression model.

Note that, if $\theta = [\theta_1, \theta_2, \dots, \theta_K]^T$ and $\phi(x) = [\phi_1(x), \phi_2(x), \dots, \phi_K(x)]^T$, then

$$
    f_\theta(x) = \phi^T(x) \theta
$$

which implies that $f_\theta(x)$ is a linear function in $\theta$, for any choice of $\phi_j(x)$.

A classical choice for the feature vector, is $\phi_j(x) = x^{j-1}$. In this way, for a given $K>0$, $\phi(x)$ represents the vector of the first $K$ monomial in $x$, i.e.
$$
    \phi(x) = [1, x, x^2, \dots, x^{K-1}]^T
$$
and $f_\theta(x)$ is a $K-1$-th degree polynomial. For this reason, $f_\theta(x)$ is called **polynomial regression model** for this choice of $\phi_j(x)$. To show the improvement of a polynomial regression model against a linear model, this is the resulting model when we use a polynomial regression model of degree 3 on the data above:
![[regression_example4.png]]
To understand how to train the parameters $\theta$ of a linear / polynomial regression model, we have to move to the next post.


---
- [[SMMAI#^homework-list-SMMAI|Homework assignments]]
- [[SMMAI#^lecture-list-smmai-lab|Lab lecture list]]